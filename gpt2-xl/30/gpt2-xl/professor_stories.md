Once upon a time, there was a professor named Richard C. Feynman, whose name I've written many times here. Professor Feynman was a man of many remarkable talents, but above all he was a brilliant mathematician. His special talent was not in arithmetic, but in logic, and it wasn't long before the world learned what we already knew: no two parts could ever be precisely the same.

At a time when computers were primitive (they used a small series of fixed switches, no bigger than a pin head), Feynman was one of the pioneers of computer-aided computation. For years, he explored the concept of "recursion," the fundamental tool in all the branches of mathematics, from geometry to combinatorics. The idea of the recursive function is that there exists a function defined to recursively build itself.

The simplest example of the recursive function is the binary tree. Let's take a simple binary tree, one whose nodes could take on any value between 0 and 2:

For example, the binary root of this tree is called the "left child." The "right child" is the reverse of the left child. The "left child's left child" is the reverse of the left child's right child. The "right child's right child" is the reverse of the right child's left child.

In this tree, all nodes are represented by integers between 0 and 1, the child nodes are 2 through